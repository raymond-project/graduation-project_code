import os
import re
import time
import json
import pandas as pd
from openai import OpenAI

# åˆå§‹åŒ– OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

FINAL_CODES = {"000","001~979","980","987","988","990","991"}

def normalize_node(node: str) -> str:
    if not node:
        return node
    node = node.strip().lower()
    if node.startswith("code "):
        node = node.replace("code ", "")
    if node.startswith("code:"):
        node = node.replace("code:", "")
    node = node.strip()
    # å˜—è©¦è½‰æ›æˆåˆæ³•çµ‚ç«¯ç¢¼
    node = normalize_final_code(node)
    return node


def safe_json_parse(s: str):
    """ä¿éšªçš„ JSON parserï¼Œé¿å…æ¨¡å‹è¼¸å‡ºä¸ä¹¾æ·¨"""
    try:
        return json.loads(s)
    except json.JSONDecodeError:
        match = re.search(r"\[.*\]", s, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        else:
            print("âš ï¸ æ¨¡å‹è¼¸å‡ºé JSONï¼š", s[:200])
            return []

def log_error(idx, report_text, error):
    """ç´€éŒ„å¤±æ•—æ¡ˆä¾‹"""
    with open("error_log.txt", "a", encoding="utf-8") as logf:
        logf.write(f"âŒ ç¬¬ {idx+1} ç­†å¤±æ•—: {error}\n")
        logf.write(f"å ±å‘Šå…§å®¹: {report_text[:200]}...\n\n")

def call_openai_with_retry(prompt, idx, report_text, retries=3, timeout=60.0):
    """å‘¼å« GPTï¼Œå…§å»º retry æ©Ÿåˆ¶"""
    for i in range(retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                timeout=timeout
            )
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {idx+1} ç­†ï¼Œç¬¬ {i+1} æ¬¡å¤±æ•—: {e}")
            if i == retries - 1:
                log_error(idx, report_text, str(e))
            time.sleep(2 ** i)
    return None

# ==============================
# è¦å‰‡
# ==============================

def build_single_chain(report_text, evidence_sentence, correct_code, idx=0):
    rules = """
    ã€Surgical Margin Distance Coding Rulesã€‘
    - 000: æ‰‹è¡“åˆ‡ç·£é™½æ€§ã€‚ç—…ç†å ±å‘Šæè¿°ã€Œinvolvedã€ã€‚å°æ–¼1mmï¼Œæ‰‹è¡“åˆ‡ç·£æ˜ç¤ºç‚ºé™½æ€§ã€‚
    - 001~979: æ‰‹è¡“åˆ‡ç·£ç‹€æ…‹ç‚ºé™°æ€§ï¼Œå‰‡è¨˜éŒ„å¯¦éš›æ‰‹è¡“åˆ‡ç·£è·é›¢ï¼Œä»¥0.1mmç‚ºå–®ä½ã€‚å¦‚:10 mm=100, 0.1 mm=001ã€‚
    - 980: æ‰‹è¡“åˆ‡ç·£è·é›¢å¤§æ–¼98mmã€‚
    - 987: åƒ…æè¿° very closeã€may not be freeï¼Œä¸”æœªæè¿°åˆ‡ç·£è·é›¢ã€‚
    - 988: ä¸é©ç”¨ã€‚æœªåŸ·è¡ŒåŸç™¼è…«ç˜¤éƒ¨ä½æ‰‹è¡“ï¼Œæˆ–ç—…ç†å ±å‘Šè¨»æ˜ç„¡æ³•è©•ä¼°ã€‚
    - 990: å†åˆ‡é™¤å¾Œç„¡æ®˜é¤˜è…«ç˜¤ï¼Œæˆ–å‰å°æ€§æ²»ç™‚å¾Œæ‰‹è¡“æ¨™æœ¬é¡¯ç¤ºç„¡æ®˜é¤˜è…«ç˜¤ã€‚
    - 991: æ‰‹è¡“é‚Šç·£ç‚ºéä¾µè¥²ç™Œ (åŸä½ç™Œæˆ–æ®˜å­˜åˆ†åŒ–ä¸è‰¯/ç•°å‹å¢ç”Ÿ)ã€‚
    """

    prompt = f"""
You are a cancer registry reasoning assistant. 
Given a pathology report and a key evidence sentence, extract a minimal ordered reasoning path 
that leads to the coding decision.

{rules}

ã€Rules for Outputã€‘
1. Always end the chain with the correct code: {correct_code}
2. If the raw report contains a distance (e.g., "60 mm"), map it to the umbrella code "001~979".
3. If the report matches special rules (TURP â†’ 988, no residual tumor â†’ 990, CIS/dysplasia â†’ 991),
   include that context before reaching the code.
4. Nodes must appear in a logical order from evidence_sentence tokens, plus any additional decisive context 
   from the full report (reportData).
5. Do not branch. Output a single linear chain.
6. Keep each node short (1â€“5 words).
7. Output JSON array only.

ã€Inputã€‘
reportData:
{report_text}

evidence_sentence:
{evidence_sentence}

Output:
Return only a valid JSON array of nodes. 
Do not include any extra text, explanation, or formatting.
"""

    resp = call_openai_with_retry(prompt, idx=idx, report_text=report_text)
    if resp is None:
        return []
    raw = resp.choices[0].message.content.strip()
    nodes = safe_json_parse(raw)

    triples = []
    for i in range(len(nodes) - 1):
        subj = normalize_node(nodes[i])
        obj = normalize_node(nodes[i + 1])
        triples.append({
            "report_id": idx,
            "subject": subj,
            "relation": "leads_to",
            "object": obj,
            "evidence_sentence": evidence_sentence
        })

    
    if triples:
        triples[-1]["object"] = correct_code

    return triples

# ==============================
# Code Normalizer
# ==============================

def normalize_final_code(val: str) -> str:
    val = val.strip()
    if val in {"000","001~979","980","987","988","990","991"}:
        return val
    try:
        num = int(val)
        if 1 <= num <= 979:
            return "001~979"
        elif num == 0:
            return "000"
        elif num >= 980:
            return "980"
    except:
        pass
    return val


# ==============================
# ä¸»ç¨‹å¼
# ==============================

if __name__ == "__main__":
    df = pd.read_csv(r"/home/st426/system/global_graph/é æ¸¬çµæœ_æ­£ç¢º_surdis.csv")

    big_graph = []
    output_file = "surgical_margin_graph.json"

    for idx, row in df.iterrows():
        report_text = row["reportData"]
        raw_code = str(row["åŸç™¼éƒ¨ä½æ‰‹è¡“åˆ‡ç·£è·é›¢"]).strip()
        correct_code = normalize_final_code(raw_code)
        evidence_sentence = row["sentence"]

        triples = build_single_chain(report_text, evidence_sentence, correct_code, idx)
        big_graph.extend(triples)

        # å³æ™‚æ›´æ–°å¤§åœ– JSON
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(big_graph, f, indent=2, ensure_ascii=False)

        print(f"âœ… å·²è™•ç†ç¬¬ {idx+1} ç­†, ç´¯ç©ä¸‰å…ƒçµ„ {len(big_graph)} æ¢ (å·²å³æ™‚å¯«å…¥ {output_file})")

    print(f"ğŸ‰ å¤§åœ–å·²å®Œæˆï¼Œè¼¸å‡ºåˆ° {output_file}")
    print("ğŸ“„ è‹¥æœ‰å¤±æ•—ï¼Œè«‹æŸ¥çœ‹ error_log.txt")
