{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e991ac78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ee2010c5fa491e92b1ccf4f0135a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_device_map: {'': 'cuda:0'}\n",
      "any param device: cuda:0\n",
      "cuda available: True\n",
      "GPU: NVIDIA TITAN V\n",
      "Capability: (7, 0)\n",
      "GPU memory: 11.77166748046875 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = r\"/home/st426/system/global_graph/Biomistral-Calme-Instruct-7b\"\n",
    "\n",
    "# 4bit 量化設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,   \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "device_map = {\"\": \"cuda:0\"}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,        \n",
    "    torch_dtype=torch.float16,   \n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "print(\"hf_device_map:\", getattr(model, \"hf_device_map\", None))\n",
    "print(\"any param device:\", next(model.parameters()).device)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
    "print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1024**3, \"GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_template failed, falling back to [INST]: Conversation roles must alternate user/assistant/user/assistant/...\n"
     ]
    }
   ],
   "source": [
    "# ==== English-only test: surgical margin extraction (robust) ====\n",
    "import re, json, torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from contextlib import nullcontext\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from jinja2 import TemplateError\n",
    "\n",
    "if getattr(tokenizer, \"pad_token_id\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "rules_dict = {\n",
    "    \"0\": \"Positive surgical margin. Pathology report describes 'involved'. Less than 1 mm, margin explicitly reported as positive.\",\n",
    "    \"1~979\": \"Negative surgical margin. Record the actual margin distance in 0.1 mm units. Example: 10 mm = 100, 0.1 mm = 001.\",\n",
    "    \"980\": \"Margin distance greater than 98 mm.\",\n",
    "    \"987\": \"Described only as 'very close' or 'may not be free', but no exact margin distance provided.\",\n",
    "    \"988\": \"Not applicable: No primary tumor resection performed.\",\n",
    "    \"990\": \"No residual tumor after re-resection, margin distance unclear.\",\n",
    "    \"991\": \"Surgical margin described as non-invasive carcinoma (carcinoma in situ or similar).\",\n",
    "    \"999\": \"Unknown if patient received primary tumor resection or margin distance not documented.\"\n",
    "}\n",
    "\n",
    "test_report =\"\"\"\n",
    "\n",
    "Pathologic diagnosis: Pathologic diagnosis: Lung  right lower lobe  permanent section of frozen section --- 1. Lung  right lower lobe  wedge resection ---- Adenocarcinoma  acinar predominant (acinar pattern: 90%; lepidic pattern: 10 %). The Atypical glands. resection margin and visceral pleura are free of tumor. 2. Lung  right lower lobe 2  wedge resection ---- Intrapulmonary Tentative frozen section diagnosis: Atypical glands. lymph node (0/1). 3. Lymph node  group 2  dissection ---- Anthracosis and Gross description: granulomatous inflammation  no tumor seen (0/4). The specimen consists of a piece of tan soft tissue  0.1x0.1x0.1 cm  4. Lymph node  group 4  dissection ---- Anthracosis and labeled as \"\"\"\"right lower lobe\"\"\"\" for frozen section. The tentative granulomatous inflammation  no tumor seen (0/8). frozen section diagnosis is \"\"\"\"Atypical glands./An\"\"\"\". All for section. 5. Lymph node  group 7  dissection ---- Anthracosis and granulomatous inflammation  no tumor seen (0/12). 6. Lymph node  group 9  dissection ---- Anthracosis  no tumor seen Microscopic description: Sections show pulmonary tissue with atypical glands. (0/5). #T-28000_2 #M-09350_2 2 3082 VT000F Ancillary study for diagnosis: 1. Special stains for VVG (no destruction) done for section B. 2. Immunohistochemical stains for thyroid transcription factor-1 (+) done for section B. 3. PD-L1 IHC staining done for section B  result: <1 % positive (1+: <1%  2+: 0%  3+: 0%) Prognostic and predictive factor: 1. Total tumor size/Invasive tumor size (required only if invasive nonmucinous adenocarcinomas with lepidic component is present): 1.1 cm/ 0.9cm. 2. Tumor focality: Single tumor. 3. Direct Invasion of Adjacent Structures: Not applicable. 4. Lymphovascular invasion: Not identified. 5. Perineural invasion: Not identified. 6. Surgical margin: (1) Bronchial Margin: Not applicable. (2) Vascular Margin: Not applicable. (3) Parenchymal Margin: Uninvolved by invasive carcinoma. 7. Visceral Pleural status: Uninvolved (PL0). 8. Regional lymph nodes: Number of Lymph Nodes Involved: 0/Number of Lymph Nodes Examined: 30. 9. Extranodal extension: Not identified. 10. Treatment effect: Not applicable. 11. Spread Through Air Spaces (STAS): Not identified. 12. Pathological TNM stage: pT1aN0 (According to the eighth edition  American Joint Committee on Cancer Staging Guidelines for Tumors). 13. TNM descriptors: Not applicable. Gross description: The specimen consists of 1) \"\"\"\"RLL\"\"\"\"  a wedge-shaped lung tissue  4.5x3x2.5 cm. One incision is noted over the visceral pleura with exposure of a tan soft to firm tumor  1.1x0.9 cm. The lesion is localized near the visceral pleura. The distance of tumor to parenchymal margin is 10 mm grossly. The residual lung parenchyma is grossly unremarkable. 2) \"\"\"\"RLL 2\"\"\"\"  a wedge-shaped lung tissue  3.5x1x1 cm. One incision is noted over the visceral pleura with exposure of a tan soft to firm nodule  0.7x0.5 cm. The lesion is localized near the visceral pleura. The distance of tumor to parenchymal margin is 5 mm grossly. 3) LN2  3 pieces of tan brown black soft to firm tissue  up to 2x1.3x1.3 cm. 4) LN4  2 pieces of tan brown black soft to firm tissue  up to 2.5x1x1 cm. 5) LN7  3 pieces of tan brown black soft to firm tissue  up to 3.5x2.5x1 cm. 6) LN9  4 pieces of tan brown black soft to firm tissue  up to 1x1x1 cm. Representative sections taken: A) parenchymal margin of specimen 1 B-C) lesion of specimen 1 D) lung parenchyma of specimen 1 E) parenchymal margin of specimen 2 F) all lung tissue of specimen 2 G) specimen 3 H) specimen 4 J) specimen 5 K) specimen 6. Note: 1. Material: Specimen: Formalin-fixed paraffin embedded (10% neutral buffered formalin). Time to fixation: Between 6 and 72 hours. 2. Method: Clone: SP263 monoclonal antibody. Staining System: Ventana medical system. 3. This case has been peer reviewed by two doctors. #T-28400_2 #M-81403_2 2 1103 00000F #T-C4300_2 #D2-53020_2 0 1100 000000\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rules_text = \"\\n\".join([f\"{c}: {desc}\" for c, desc in rules_dict.items()])\n",
    "\n",
    "\n",
    "system_msg = (\n",
    "    \"You are a pathology coding assistant. Respond with ONLY a valid JSON array.\"\n",
    "    \" Do not include any explanations or extra text.\"\n",
    "    \" Begin your answer with '[' and end it with ']'.\"\n",
    ")\n",
    "user_msg = f\"\"\"\n",
    "Here are the surgical margin coding rules:\n",
    "{rules_text}\n",
    "\n",
    "Report (already pre-filtered for margin-related sentences):\n",
    "{test_report}\n",
    "\n",
    "Instructions:\n",
    "- Read the report carefully.\n",
    "- Only output sentences that mention surgical margins or distances.\n",
    "- For each, create an object with:\n",
    "  - \"sentence\": the exact sentence\n",
    "  - \"distance_raw\": the numeric distance if present (e.g. \"9 mm\", \"0.5 cm\"), otherwise null\n",
    "  - \"code\": assign the proper rule code (from the list above)\n",
    "\n",
    "Output requirements:\n",
    "- Output MUST be ONLY a valid JSON array.\n",
    "- Each element must follow: {{\"sentence\": \"...\", \"code\": \"...\"}}.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(tokenizer: PreTrainedTokenizerBase, system_msg: str, fewshot_json: str, user_msg: str) -> str:\n",
    "    # messages：user/assistant/user 交替；system 放第一個\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": \"Follow the instructions and reply ONLY with a valid JSON array.\"},\n",
    "        {\"role\": \"assistant\", \"content\": fewshot_json},  \n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "    try:\n",
    "        if getattr(tokenizer, \"chat_template\", None):\n",
    "            return tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        raise TemplateError(\"no chat_template\")\n",
    "    except Exception as e:\n",
    "        print(\"chat_template failed, falling back to [INST]:\", e)\n",
    "        # Fallback：Mistral/Llama 指令格式\n",
    "        return (\n",
    "            f\"<s>[INST] {system_msg}\\n\\n\"\n",
    "            f\"{fewshot_json}\\n\\n\"\n",
    "            f\"{user_msg} [/INST]\"\n",
    "        )\n",
    "\n",
    "\n",
    "fewshot_json = \"\"\"[\n",
    "  {\"sentence\": \"Example sentence.\", \"code\": \"10\"}\n",
    "]\"\"\"\n",
    "\n",
    "prompt_text = build_prompt(tokenizer, system_msg, fewshot_json, user_msg)\n",
    "\n",
    "\n",
    "\n",
    "class StopOnValidJsonArray(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, prompt_len):\n",
    "        self.tok = tokenizer\n",
    "        self.prompt_len = prompt_len\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        gen_ids = input_ids[0][self.prompt_len:]\n",
    "        text = self.tok.decode(gen_ids, skip_special_tokens=True)\n",
    "        m = re.search(r\"\\[\\s*(?:.|\\n)*\\]\", text)\n",
    "        if not m:\n",
    "            return False\n",
    "        try:\n",
    "            json.loads(m.group(0))\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def extract_json_only(gen_text: str):\n",
    "    m = re.search(r\"\\[\\s*(?:.|\\n)*\\]\", gen_text)\n",
    "    if not m:\n",
    "        return []\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "stopper = StopOnValidJsonArray(tokenizer, prompt_len=inputs[\"input_ids\"].shape[1])\n",
    "\n",
    "sdpa_ctx = nullcontext()\n",
    "if hasattr(torch.backends.cuda, \"sdp_kernel\"):\n",
    "    sdpa_ctx = torch.backends.cuda.sdp_kernel(\n",
    "        enable_flash=False, enable_mem_efficient=False, enable_math=True\n",
    "    )\n",
    "if hasattr(torch.backends.cuda, \"enable_flash_sdp\"):\n",
    "    torch.backends.cuda.enable_flash_sdp(False)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "with torch.no_grad(), sdpa_ctx:\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=StoppingCriteriaList([stopper]),\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(gen_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
