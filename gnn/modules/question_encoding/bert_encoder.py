
import torch.nn.functional as F
import torch.nn as nn
import os
import torch 

VERY_SMALL_NUMBER = 1e-10
VERY_NEG_NUMBER = -100000000000


from transformers import AutoModel, AutoTokenizer #DistilBertModel, BertModel, BertTokenizer, RobertaModel, RobertaTokenizer
from torch.nn import LayerNorm
import warnings
warnings.filterwarnings("ignore")


from .base_encoder import BaseInstruction


class BERTInstruction(BaseInstruction):

    def __init__(self, args, word_embedding, num_word, model, constraint=False):
        super(BERTInstruction, self).__init__(args, constraint)
        self.word_embedding = word_embedding
        self.num_word = num_word
        self.constraint = constraint
        
        entity_dim = self.entity_dim
        self.model = model
        
        
        if model == 'biobert-v1.1':
            model_path = os.path.join('/home/st426/system/GNN-RAG/gnn/bert_model', model)
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            self.pretrained_weights = model_path
            word_dim = 768    # ä¾†æºæ–¼ configä¸­çš„ "hidden_size" 
            
        elif model == 'scibert-nli':
            model_path = os.path.join('/home/st426/system/GNN-RAG/gnn/bert_model', model)
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            self.pretrained_weights = model_path
            word_dim = 768    # ä¾†æºæ–¼ configä¸­çš„ "hidden_size
        elif model == 'ClinicalBERT':
            model_path = os.path.join('/home/st426/system/GNN-RAG/gnn/bert_model', model)
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            self.pretrained_weights = model_path
            word_dim = 768    # ä¾†æºæ–¼ configä¸­çš„ "hidden_size"  

        #elif model == '':  çœ‹ä¹‹å¾Œæœƒä¸æœƒ æœ‰å…¶ä»–bertæ¨¡å‹æˆ‘å›ºå®šéƒ½æ”¾åœ¨GNN-RAG/gnn/bert_modelä¸‹ ä¾æ“šæ¨¡å‹åç¨±ç‚ºæª”å
        else: 
            raise ValueError(f"æ²’æœ‰é€™æ¨¡å‹: {model}")
        
        
        
        #pad_val æ˜¯å–å¾— tokenizer çš„ padding token IDï¼Œç”¨ä¾†åš attention mask
        #word_dim æ˜¯èªè¨€æ¨¡å‹è¼¸å‡ºå‘é‡çš„ç¶­åº¦ï¼Œé€šå¸¸ç‚º 768ï¼ˆæˆ– 384, 1024 ç­‰ï¼‰from config
        self.pad_val = self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)
        self.word_dim = word_dim
        print('word_dim', self.word_dim)
        
        
    
        #é€™å…©å±¤æ˜¯ç”¨ä¾†å¾ŒçºŒé€²è¡Œæ³¨æ„åŠ›è¨ˆç®—èˆ‡èåˆï¼ˆå¦‚ query-entity matchingï¼‰ç”¨é€”çš„ linear å±¤ã€‚ 
        self.cq_linear = nn.Linear(in_features=4 * entity_dim, out_features=entity_dim)
        self.ca_linear = nn.Linear(in_features=entity_dim, out_features=1)
        
        
        #ç‚ºæ¯å€‹ instruction æ­¥é©ŸåŠ ä¸Šç¨ç«‹çš„ç·šæ€§å±¤ï¼ˆå¯æƒ³æˆç”¨ä¾†æ›´æ–° query å‘é‡çš„æ¨¡çµ„ï¼‰
        for i in range(self.num_ins):
            self.add_module('question_linear' + str(i), nn.Linear(in_features=entity_dim, out_features=entity_dim))
            
            
        #æŠŠå¾ BERT è¼¸å‡ºçš„ word-level embedding è½‰æˆæ¨¡å‹å…§éƒ¨ç”¨çš„å‘é‡ç©ºé–“ï¼ˆå¾ word_dim æ˜ å°„åˆ° entity_dim
        self.question_emb = nn.Linear(in_features=word_dim, out_features=entity_dim)
        
        
        
        #è¼‰å…¥æ¨¡å‹æœ¬é«” --lm æœ‰bertçš„è©±é€™å°±æœƒTrue
        if not self.constraint:
            self.encoder_def()







#ä»¥ä¸‹ç‚ºæˆ‘é‚„æ²’çœ‹ç´°ç¯€
#encoder_def = æŠŠæ¨¡å‹è¼‰å¥½
#encode_question = æ‹¿æ¨¡å‹ä¾†ç·¨ç¢¼å•é¡Œæ–‡å­—
    def encoder_def(self):
        # initialize entity embedding
        word_dim = self.word_dim
        entity_dim = self.entity_dim
        self.node_encoder = AutoModel.from_pretrained(self.pretrained_weights)
        print('ç¸½å…±çš„ Params', sum(p.numel() for p in self.node_encoder.parameters()))
        
        
        
        #æ‡‰è©²æ˜¯ä¸ç”¨ç”¨åˆ°LM params å› ç‚ºé å…ˆè¨“ç·´æ¨¡å‹å°±å¾ˆå¥½ç”¨
        
        #åªè¨“ç·´ä½  GNN æ¨¡å‹ 
        if self.lm_frozen == 1:
            print('å‡çµ LM params')
            for param in self.node_encoder.parameters():
                param.requires_grad = False
        
        
        
        
        #æ•´å€‹ BERT/BioBERT æ¨¡å‹æœƒè·Ÿè‘— GNN ä¸€èµ·å¾®èª¿
        #å¦‚æœç”¨é€™fine-tuingå¾Œæ‡‰è©²å¯ä»¥é©æ‡‰ä¸‹æ¸¸ ä½†æ˜¯ä¹‹å¾Œè¦è¨˜å¾—è¦é‡æ–°gitä¸€æ¬¡æ¨¡å‹
        else:
            for param in self.node_encoder.parameters():
                param.requires_grad = True
            print('ä¸å‡çµ LM params')
            
            

    def encode_question(self, query_input, store=True):
        outputs = self.node_encoder(**query_input)
        hidden_emb = outputs.last_hidden_state  # (batch, seq_len, 768)
        hidden_emb = torch.nan_to_num(hidden_emb, nan=0.0, posinf=0.0, neginf=0.0)

        # log question_emb çš„æ¬Šé‡ norm
        if hasattr(self, "question_emb"):
            print("[DEBUG][encode_question] question_emb weight norm:",
                self.question_emb.weight.norm().item())

        # CLS pooling
        cls_emb = hidden_emb[:, 0:1, :]  # (batch, 1, 768)
        query_node_emb = self.question_emb(cls_emb)  # (batch, 1, entity_dim)
        query_node_emb = torch.nan_to_num(query_node_emb, nan=0.0, posinf=0.0, neginf=0.0)

        if store:
            # âœ… å­˜ä¸‹ä¾†ï¼Œçµ¦ get_instruction ç”¨
            self.query_hidden_emb = self.question_emb(hidden_emb)
            self.query_node_emb = query_node_emb
            self.query_mask = query_input["attention_mask"].float()

            print("[DEBUG][encode_question] store=True | hidden_emb:", hidden_emb.shape,
                "query_hidden_emb:", self.query_hidden_emb.shape,
                "query_node_emb:", self.query_node_emb.shape)
            return self.query_hidden_emb, self.query_node_emb
        else:
            # ğŸš¨ relation encoding: åªå› CLS vector
            print("[DEBUG][encode_question] store=False | return query_node_emb:", query_node_emb.shape)
            return query_node_emb.squeeze(1)

